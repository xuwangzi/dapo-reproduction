# Open R1

*å®Œå…¨å¼€æºçš„DeepSeek-R1å¤ç°é¡¹ç›®ã€‚è¿™ä¸ªä»“åº“æ­£åœ¨å¼€å‘ä¸­ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ„å»ºå®ƒï¼*

**ç›®å½•**  
1. [æ¦‚è¿°](#æ¦‚è¿°)  
2. [æ”»å‡»è®¡åˆ’](#æ”»å‡»è®¡åˆ’)  
3. [å®‰è£…](#å®‰è£…)  
4. [è®­ç»ƒæ¨¡å‹](#è®­ç»ƒæ¨¡å‹)  
   - [SFT](#sft)  
   - [GRPO](#grpo)  
5. [è¯„ä¼°æ¨¡å‹](#è¯„ä¼°æ¨¡å‹)  
6. [å¤ç°Deepseekçš„è¯„ä¼°ç»“æœ](#å¤ç°deepseekçš„è¯„ä¼°ç»“æœ)  
7. [æ•°æ®ç”Ÿæˆ](#æ•°æ®ç”Ÿæˆ)  
   - [ä»å°å‹è’¸é¦R1æ¨¡å‹ç”Ÿæˆæ•°æ®](#ä»å°å‹è’¸é¦r1æ¨¡å‹ç”Ÿæˆæ•°æ®)  
   - [ä»DeepSeek-R1ç”Ÿæˆæ•°æ®](#ä»deepseek-r1ç”Ÿæˆæ•°æ®)  
8. [è´¡çŒ®](#è´¡çŒ®)

## æ¦‚è¿°

è¿™ä¸ªä»“åº“çš„ç›®æ ‡æ˜¯æ„å»ºR1æµæ°´çº¿ä¸­ç¼ºå¤±çš„éƒ¨åˆ†ï¼Œä½¿æ¯ä¸ªäººéƒ½èƒ½å¤ç°å¹¶åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ„å»ºã€‚è¯¥é¡¹ç›®åœ¨è®¾è®¡ä¸Šä¿æŒç®€å•ï¼Œä¸»è¦åŒ…æ‹¬ï¼š

- `src/open_r1`: åŒ…å«è®­ç»ƒæ¨¡å‹ä»¥åŠç”Ÿæˆåˆæˆæ•°æ®çš„è„šæœ¬ï¼š
    - `grpo.py`: åœ¨ç»™å®šæ•°æ®é›†ä¸Šä½¿ç”¨GRPOè®­ç»ƒæ¨¡å‹ã€‚
    - `sft.py`: åœ¨æ•°æ®é›†ä¸Šå¯¹æ¨¡å‹æ‰§è¡Œç®€å•çš„SFTã€‚
    - `generate.py`: ä½¿ç”¨[Distilabel](https://github.com/argilla-io/distilabel)ä»æ¨¡å‹ç”Ÿæˆåˆæˆæ•°æ®ã€‚
- `Makefile`: åŒ…å«åˆ©ç”¨ä¸Šè¿°è„šæœ¬è¿›è¡ŒR1æµæ°´çº¿ä¸­æ¯ä¸ªæ­¥éª¤çš„æ˜“è¿è¡Œå‘½ä»¤ã€‚

### æ”»å‡»è®¡åˆ’

æˆ‘ä»¬å°†ä½¿ç”¨DeepSeek-R1[æŠ€æœ¯æŠ¥å‘Š](https://github.com/deepseek-ai/DeepSeek-R1)ä½œä¸ºæŒ‡å—ï¼Œè¯¥æŠ¥å‘Šå¤§è‡´å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªä¸»è¦æ­¥éª¤ï¼š

* æ­¥éª¤1: é€šè¿‡ä»DeepSeek-R1è’¸é¦é«˜è´¨é‡è¯­æ–™åº“æ¥å¤åˆ¶R1-Distillæ¨¡å‹ã€‚
* æ­¥éª¤2: å¤åˆ¶DeepSeekç”¨äºåˆ›å»ºR1-Zeroçš„çº¯RLæµæ°´çº¿ã€‚è¿™å¯èƒ½æ¶‰åŠä¸ºæ•°å­¦ã€æ¨ç†å’Œä»£ç ç­–åˆ’æ–°çš„å¤§è§„æ¨¡æ•°æ®é›†ã€‚
* æ­¥éª¤3: å±•ç¤ºæˆ‘ä»¬å¯ä»¥é€šè¿‡å¤šé˜¶æ®µè®­ç»ƒä»åŸºç¡€æ¨¡å‹åˆ°RLè°ƒä¼˜ã€‚

<center>
    <img src="assets/plan-of-attack.png" width="500">
</center>

## æ–°é—» ğŸ—ï¸

* **ğŸ§‘â€ğŸ³ [2025/05/26] (æ­¥éª¤1å®Œæˆ!)** æˆ‘ä»¬å‘å¸ƒäº†[**Mixture-of-Thoughts**](https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts)â€”â€”ä¸€ä¸ªç»è¿‡ç²¾å¿ƒç­–åˆ’çš„æ¨ç†æ•°æ®é›†ï¼ŒåŒ…å«ä»R1è’¸é¦å‡ºçš„35ä¸‡ä¸ªéªŒè¯è½¨è¿¹ã€‚è¯¥æ•°æ®é›†æ¶µç›–æ•°å­¦ã€ç¼–ç¨‹å’Œç§‘å­¦ä»»åŠ¡ï¼Œæ—¨åœ¨æ•™å¯¼è¯­è¨€æ¨¡å‹é€æ­¥æ¨ç†ã€‚æˆ‘ä»¬è¿˜æä¾›äº†è®­ç»ƒ[OpenR1-Distill-7B](https://huggingface.co/open-r1/OpenR1-Distill-7B)çš„é…æ–¹ï¼Œå®ƒå¤åˆ¶äº†[deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)çš„æ¨ç†èƒ½åŠ›ï¼Œæ ‡å¿—ç€Open R1é¡¹ç›®æ­¥éª¤1çš„å®Œæˆã€‚
* **âš¡ï¸ [2025/03/11] [(æ›´æ–° #3)](https://huggingface.co/blog/open-r1/update-3):** æˆ‘ä»¬å‘å¸ƒäº†[**CodeForces-CoTs**](https://huggingface.co/datasets/open-r1/codeforces-cots)æ•°æ®é›†ï¼ŒåŒ…å«1ä¸‡ä¸ªç«æŠ€ç¼–ç¨‹é—®é¢˜å’Œ10ä¸‡ä¸ªä»R1è’¸é¦çš„è§£å†³æ–¹æ¡ˆã€‚æˆ‘ä»¬è¿˜å‘å¸ƒäº†IOI24ï¼šä¸€ä¸ªç”±å›½é™…å¥¥æ—åŒ¹å…‹_éå¸¸_å›°éš¾é—®é¢˜ç»„æˆçš„æ–°åŸºå‡†ã€‚åœ¨CodeForces-CoTsä¸Šè®­ç»ƒçš„7B Qwenæ¨¡å‹åœ¨IOI24ä¸Šå¯ä»¥è¶…è¶ŠClaude 3.7 Sonnetï¼Œè€Œ32Bæ¨¡å‹å¯ä»¥è¶…è¶ŠR1æœ¬èº«ã€‚
* **âˆ [2025/02/10] [(æ›´æ–° #2)](https://huggingface.co/blog/open-r1/update-2):** æˆ‘ä»¬å‘å¸ƒäº†[**OpenR1-Math-220k**](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k)æ•°æ®é›†ï¼ŒåŒ…å«ä»R1åœ¨æ–°ç‰ˆæœ¬NuminaMathä¸Šè’¸é¦çš„22ä¸‡ä¸ªè½¨è¿¹ã€‚åœ¨æ­¤æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹ä¸DeepSeekçš„è’¸é¦æ¨¡å‹æ€§èƒ½ç›¸åŒ¹é…ã€‚
* **ğŸ”¥ [2025/02/02] [(æ›´æ–° #1)](https://huggingface.co/blog/open-r1/update-1):** æˆ‘ä»¬å®ç°äº†[è®­ç»ƒ](https://github.com/huggingface/open-r1?tab=readme-ov-file#training-models)ã€[æ¨ç†](https://github.com/huggingface/open-r1?tab=readme-ov-file#data-generation)å’Œ[è¯„ä¼°](https://github.com/huggingface/open-r1?tab=readme-ov-file#reproducing-deepseeks-evaluation-results)æµæ°´çº¿çš„ç¬¬ä¸€éƒ¨åˆ†ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

## å®‰è£…

> [!CAUTION]
> åº“ä¾èµ–CUDA 12.4ã€‚å¦‚æœæ‚¨çœ‹åˆ°ä¸åˆ†æ®µé”™è¯¯ç›¸å…³çš„é”™è¯¯ï¼Œè¯·ä½¿ç”¨`nvcc --version`ä»”ç»†æ£€æŸ¥æ‚¨ç³»ç»Ÿæ­£åœ¨è¿è¡Œçš„ç‰ˆæœ¬ã€‚

è¦è¿è¡Œæ­¤é¡¹ç›®ä¸­çš„ä»£ç ï¼Œé¦–å…ˆï¼Œä½¿ç”¨ä¾‹å¦‚`uv`åˆ›å»ºä¸€ä¸ªPythonè™šæ‹Ÿç¯å¢ƒã€‚
è¦å®‰è£…`uv`ï¼Œè¯·éµå¾ª[UVå®‰è£…æŒ‡å—](https://docs.astral.sh/uv/getting-started/installation/)ã€‚

> [!NOTE]
> ä½œä¸ºå¿«æ·æ–¹å¼ï¼Œè¿è¡Œ`make install`æ¥è®¾ç½®å¼€å‘åº“ï¼ˆè¯¦ç»†è¯´æ˜å¦‚ä¸‹ï¼‰ã€‚ä¹‹åï¼Œå¦‚æœä¸€åˆ‡è®¾ç½®æ­£ç¡®ï¼Œæ‚¨å¯ä»¥å°è¯•Open-R1æ¨¡å‹ã€‚

```shell
uv venv openr1 --python 3.11 && source openr1/bin/activate && uv pip install --upgrade pip
```

> [!TIP]
> å¯¹äºHugging Faceé›†ç¾¤ç”¨æˆ·ï¼Œåœ¨æ‚¨çš„`.bashrc`ä¸­æ·»åŠ `export UV_LINK_MODE=copy`ä»¥æŠ‘åˆ¶æ¥è‡ª`uv`çš„ç¼“å­˜è­¦å‘Š

æ¥ä¸‹æ¥ï¼Œå®‰è£…vLLMå’ŒFlashAttentionï¼š

```shell
uv pip install vllm==0.8.5.post1
uv pip install setuptools && uv pip install flash-attn --no-build-isolation
```

è¿™ä¹Ÿå°†å®‰è£…PyTorch `v2.6.0`ï¼Œä½¿ç”¨è¿™ä¸ªç‰ˆæœ¬æ˜¯**éå¸¸é‡è¦çš„**ï¼Œå› ä¸ºvLLMäºŒè¿›åˆ¶æ–‡ä»¶æ˜¯ä¸ºå®ƒç¼–è¯‘çš„ã€‚ç„¶åæ‚¨å¯ä»¥é€šè¿‡`pip install -e .[LIST OF MODES]`ä¸ºæ‚¨çš„ç‰¹å®šç”¨ä¾‹å®‰è£…å…¶ä½™ä¾èµ–é¡¹ã€‚å¯¹äºå¤§å¤šæ•°è´¡çŒ®è€…ï¼Œæˆ‘ä»¬æ¨èï¼š

```shell
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e ".[dev]"
```

æ¥ä¸‹æ¥ï¼Œç™»å½•æ‚¨çš„Hugging Faceå’ŒWeights and Biasesè´¦æˆ·ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
huggingface-cli login
wandb login
```

æœ€åï¼Œæ£€æŸ¥æ‚¨çš„ç³»ç»Ÿæ˜¯å¦å®‰è£…äº†Git LFSï¼Œä»¥ä¾¿æ‚¨å¯ä»¥åŠ è½½å’Œæ¨é€æ¨¡å‹/æ•°æ®é›†åˆ°Hugging Face Hubï¼š

```shell
git-lfs --version
```

å¦‚æœæœªå®‰è£…ï¼Œè¿è¡Œï¼š

```shell
sudo apt-get install git-lfs
```

## è®­ç»ƒæ¨¡å‹

> [!NOTE]
> ä¸‹é¢çš„è®­ç»ƒå‘½ä»¤æ˜¯ä¸º8 x H100s (80GB)èŠ‚ç‚¹é…ç½®çš„ã€‚å¯¹äºä¸åŒçš„ç¡¬ä»¶å’Œæ‹“æ‰‘ï¼Œæ‚¨å¯èƒ½éœ€è¦è°ƒæ•´æ‰¹é‡å¤§å°å’Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚

æˆ‘ä»¬æ”¯æŒä½¿ç”¨DDPæˆ–DeepSpeed (ZeRO-2å’ŒZeRO-3)è®­ç»ƒæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè¦åœ¨ä»DeepSeek-R1è’¸é¦çš„æ•°æ®é›†ä¸Šæ‰§è¡ŒSFTï¼Œå…¶ä¸­åŒ…å«æ¨ç†è½¨è¿¹å¦‚[open-r1/Mixture-of-Thoughts](https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts)ï¼Œè¿è¡Œï¼š

```shell
# é€šè¿‡å‘½ä»¤è¡Œè®­ç»ƒ
accelerate launch --config_file=recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --model_name_or_path open-r1/Qwen2.5-Math-7B-RoPE-300k \
    --dataset_name open-r1/Mixture-of-Thoughts \
    --dataset_config all \
    --eos_token '<|im_end|>' \
    --learning_rate 4.0e-5 \
    --num_train_epochs 5 \
    --max_seq_length 32768 \
    --per_device_train_batch_size 2 \
    --gradient_checkpointing \
    --bf16 \
    --use_liger_kernel \
    --output_dir data/OpenR1-Distill-7B

# é€šè¿‡YAMLé…ç½®è®­ç»ƒ
accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml
```

ç›®å‰ï¼Œæ”¯æŒä»¥ä¸‹ä»»åŠ¡ï¼š

* ç›‘ç£å¾®è°ƒ `sft`
* ç¾¤ä½“ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– `grpo`

> [!TIP]
> å¦‚æœæ‚¨å¢åŠ /å‡å°‘GPUæ•°é‡ï¼Œæˆ‘ä»¬å»ºè®®ä¹Ÿç›¸åº”åœ°å¢åŠ æ¯è®¾å¤‡æ‰¹é‡å¤§å°æˆ–æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼Œä»¥ä¿æŒå…¨å±€æ‰¹é‡å¤§å°ä¸å˜ã€‚

é»˜è®¤æƒ…å†µä¸‹ï¼Œè¿™äº›è„šæœ¬ä¼šå°†æ¯ä¸ªæ¨¡å‹æ¨é€åˆ°æ‚¨çš„Hugging Face Hubç”¨æˆ·åï¼Œå³`{username}/{model_name}-{task}`ã€‚æ‚¨å¯ä»¥é€šè¿‡å°†å‚æ•°é™„åŠ åˆ°å‘½ä»¤ä¸­æ¥è¦†ç›–æ¯ä¸ªYAMLé…ç½®ä¸­çš„å‚æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
# å°†åŸºç¡€æ¨¡å‹æ›´æ”¹ä¸ºè¾ƒå°çš„å˜ä½“
accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml \
    --model_name_or_path Qwen/Qwen3-0.6B-Base \
    --hub_model_id OpenR1-Distill-0.6B \
    --output_dir data/OpenR1-Distill-0.6B
```

å¦‚æœæ‚¨è¿˜å¸Œæœ›è¦†ç›–Weights and Biasesé»˜è®¤è®¾ç½®ï¼Œå¯ä»¥å¦‚ä¸‹æ“ä½œï¼š

```shell
accelerate launch --config_file recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml
    --wandb_entity huggingface --wandb_project open-r1 --run_name Qwen2.5-1.5B-GRPO
```

**ğŸš¨ è­¦å‘Š ğŸš¨**

å¤§å¤šæ•°åŸºç¡€æ¨¡å‹å¦‚`meta-llama/Llama-3.2-1B`æ²¡æœ‰èŠå¤©æ¨¡æ¿ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è®­ç»ƒæœŸé—´è®¾ç½®ChatMLä½œä¸ºé»˜è®¤å€¼ã€‚ä½†æ˜¯ï¼Œå¯¹äºQwenåŸºç¡€æ¨¡å‹å¦‚`Qwen/Qwen2.5-1.5B`ï¼ŒèŠå¤©æ¨¡æ¿åœ¨åˆ†è¯å™¨ä¸­æ˜¯é¢„å®šä¹‰çš„ï¼Œæ‰€ä»¥å¿…é¡»ç›¸åº”åœ°è®¾ç½®EOSä»¤ç‰Œï¼Œä¾‹å¦‚

```diff
# ä¸ºQwenåŸºç¡€æ¨¡å‹å°†EOSä»¤ç‰Œä¸èŠå¤©æ¨¡æ¿å¯¹é½
accelerate launch --config_file=recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --model_name_or_path Qwen/Qwen2.5-1.5B \
+   --eos_token '<|im_end|>'
    --dataset_name open-r1/Mixture-of-Thoughts \
    --dataset_config all \
    --learning_rate 4.0e-5 \
    --num_train_epochs 1 \
    --max_seq_length 32768 \
    --per_device_train_batch_size 16 \
    --gradient_checkpointing \
    --bf16 \
    --use_liger_kernel \
    --output_dir data/Qwen2.5-1.5B-Open-R1-Distill
```

å¦‚æœæ‚¨å¸Œæœ›ä½¿ç”¨è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿ï¼ˆä¾‹å¦‚Llamaæˆ–Gemmaï¼‰ï¼Œåˆ™å¿…é¡»æä¾›èŠå¤©æ¨¡æ¿å’Œç›¸å…³çš„EOSä»¤ç‰Œï¼š

```diff
# å°†EOSä»¤ç‰Œä¸è‡ªå®šä¹‰èŠå¤©æ¨¡æ¿å¯¹é½
accelerate launch --config_file=recipes/accelerate_configs/zero3.yaml src/open_r1/sft.py \
    --model_name_or_path meta-llama/Llama-3.2-1B \
+   --chat_template "$(cat llama_chat_template.jinja)" \
+   --eos_token '<|eot_id|>' \
    --dataset_name open-r1/Mixture-of-Thoughts \
    --dataset_config all \
    --learning_rate 4.0e-5 \
    --num_train_epochs 1 \
    --max_seq_length 32768 \
    --per_device_train_batch_size 16 \
    --gradient_checkpointing \
    --bf16 \
    --use_liger_kernel \
    --output_dir data/Llama-3.2-1B-Open-R1-Distill
```

### SFTè’¸é¦

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé…æ–¹æ¥å¤ç°[deepseek-ai/DeepSeek-R1-Distill-Qwen-7B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B)çš„æ¨ç†èƒ½åŠ›ï¼Œä»ç›¸åŒçš„åŸºç¡€æ¨¡å‹å¼€å§‹ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿è¡Œï¼š

```shell
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file recipes/accelerate_configs/zero3.yaml \
    src/open_r1/sft.py \
    --config recipes/OpenR1-Distill-7B/sft/config_distill.yaml
```

ç»“æœå°†æ˜¯ä¸€ä¸ªç±»ä¼¼[open-r1/OpenR1-Distill-7B](https://huggingface.co/open-r1/OpenR1-Distill-7B)çš„æ¨¡å‹ï¼Œå…·æœ‰ä»¥ä¸‹ä¸‹æ¸¸æ€§èƒ½ï¼š

| æ¨¡å‹                       | AIME 2024 | MATH-500 | GPQA Diamond | LiveCodeBench v5 |
|-----------------------------|-----------|----------|--------------|------------------|
| OpenR1-Distill-7B           | 52.7      | 89.0     | 52.8         | 39.4             |
| DeepSeek-R1-Distill-Qwen-7B | 51.3      | 93.5     | 52.4         | 37.4             |

æ‚¨å¯ä»¥è°ƒæ•´YAMLé…ç½®ä»¥åœ¨ä¸åŒçš„åŸºç¡€æ¨¡å‹æˆ–æ•°æ®é›†ä¸Šè®­ç»ƒã€‚

### GRPO

æˆ‘ä»¬ä½¿ç”¨TRLçš„[vLLMåç«¯](https://huggingface.co/docs/trl/speeding_up_training?vllm+examples=GRPO#vllm-for-fast-generation-in-online-methods)æ¥æ‰©å±•è·¨å¤šä¸ªèŠ‚ç‚¹çš„å¤§å‹æ¨¡å‹è®­ç»ƒã€‚å¯¹äºè·¨8ä¸ªGPUçš„å°å‹æ¨¡å‹çš„å•èŠ‚ç‚¹è®­ç»ƒï¼Œä½¿ç”¨`vllm_mode="colocate"`åœ¨ä¸è®­ç»ƒè„šæœ¬ç›¸åŒçš„è¿›ç¨‹ä¸­è¿è¡ŒvLLMï¼š

```shell
ACCELERATE_LOG_LEVEL=info \
    accelerate launch --config_file recipes/accelerate_configs/zero3.yaml \
    src/open_r1/grpo.py --config recipes/DeepSeek-R1-Distill-Qwen-1.5B/grpo/config_demo.yaml \
    --vllm_mode colocate
```

> [!WARNING]
> è’¸é¦DeepSeekæ¨¡å‹ä¸­ä½¿ç”¨çš„èŠå¤©æ¨¡æ¿çœç•¥äº†`<think>`å’Œ`</think>`æ ‡ç­¾å†…æ¨ç†å—çš„å†…å®¹ã€‚å®ƒè¿˜ç”¨`<think>`é¢„å¡«å……åŠ©æ‰‹å“åº”ï¼Œè¿™ä¼šå¹²æ‰°æ ¼å¼å¥–åŠ±å‡½æ•°ã€‚ä¸ºäº†å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œé‡è¦çš„æ˜¯è¦è¦†ç›–èŠå¤©æ¨¡æ¿ï¼Œå¦‚åœ¨ä¾‹å¦‚[recipes/DeepSeek-R1-Distill-Qwen-1.5B/grpo/config_demo.yaml](./recipes/DeepSeek-R1-Distill-Qwen-1.5B/grpo/config_demo.yaml)ä¸­æ‰€åšçš„ã€‚

å¯¹äºN+1ä¸ªèŠ‚ç‚¹çš„å¤šèŠ‚ç‚¹è®­ç»ƒï¼Œå…¶ä¸­1ä¸ªèŠ‚ç‚¹è¿è¡ŒvLLMæœåŠ¡å™¨ï¼ŒNä¸ªèŠ‚ç‚¹è¿è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç¤ºä¾‹Slurmè„šæœ¬ã€‚ä¾‹å¦‚ï¼Œè¦åœ¨1+1ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨æ•°æ®å¹¶è¡Œè¿è¡Œä¸Šè¿°ç¤ºä¾‹ï¼Œè¿è¡Œï¼š

```shell
sbatch --nodes=2 slurm/train.slurm --model Qwen2.5-1.5B-Instruct --task grpo --config demo --accelerator zero2 --dp 8 --tp 1
```

æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[åœ¨Slurmé›†ç¾¤ä¸Šå¯åŠ¨ä½œä¸š](#åœ¨slurmé›†ç¾¤ä¸Šå¯åŠ¨ä½œä¸š)éƒ¨åˆ†ã€‚

#### GRPOæ•°æ®é›†è¿‡æ»¤

æˆ‘ä»¬æä¾›æ”¯æŒé€šè¿‡ç”Ÿæˆå’Œè®¡ç®—å¯éªŒè¯ä»»åŠ¡çš„é€šè¿‡ç‡æ¥è¿‡æ»¤æ•°æ®é›†ï¼Œè¯·å‚é˜…æ­¤[README](scripts/pass_rate_filtering/README.md)

#### ğŸ‘¨â€ğŸ’» ä½¿ç”¨ä»£ç è§£é‡Šå™¨è®­ç»ƒ

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ª`code`å¥–åŠ±å‡½æ•°ï¼Œç”¨äºåœ¨è®­ç»ƒæœŸé—´æ‰§è¡Œç­–ç•¥ç”Ÿæˆçš„ä»£ç ã€‚ç›®å‰ï¼Œæ­¤å¥–åŠ±å‡½æ•°é’ˆå¯¹ä»£ç ç«èµ›å¦‚[Codeforces](https://codeforces.com)ï¼Œå…¶ä¸­è§£å†³æ–¹æ¡ˆé’ˆå¯¹ä¸€ç»„æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œï¼Œæ€»ä½“æˆåŠŸç‡ä½œä¸ºæœ€ç»ˆå¥–åŠ±è¿”å›ã€‚ä¸ºäº†ç¡®ä¿å®‰å…¨æ‰§è¡Œï¼Œæˆ‘ä»¬æ”¯æŒå¤šä¸ªæ²™ç®±æä¾›å•†ï¼š

1. [E2B](https://e2b.dev) - å¿«é€Ÿã€åŸºäºäº‘çš„æ²™ç®±ï¼Œä¸“æ³¨äºPythonæ‰§è¡Œ
2. [Morph](https://cloud.morph.so/web/) - åŸºäºäº‘çš„æ²™ç®±ï¼Œå…·æœ‰æ›´å¹¿æ³›çš„è¯­è¨€æ”¯æŒ - Python/JS/C++/Rust

è¦ä½¿ç”¨ä»£ç å¥–åŠ±å‡½æ•°ï¼Œé¦–å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–é¡¹ï¼š

```shell
uv pip install -e '.[code]'
```

##### E2Bæä¾›å•†

è¦ä½¿ç”¨E2Bæ²™ç®±ï¼Œåˆ›å»ºä¸€ä¸ª`.env`æ–‡ä»¶å¹¶æ·»åŠ æ‚¨çš„E2B APIä»¤ç‰Œï¼š

```
E2B_API_KEY="e2b_xxx"
```

##### Morphæä¾›å•†

è¦ä½¿ç”¨Morphï¼Œé¦–å…ˆå®‰è£…morphcloudåŒ…ï¼š

```shell
pip install morphcloud
```

ç„¶åå°†æ‚¨çš„Morph APIä»¤ç‰Œæ·»åŠ åˆ°`.env`æ–‡ä»¶ï¼š

```
MORPH_API_KEY="YOUR_MORPH_API_KEY"
```

è¦æŒ‡å®šä½¿ç”¨å“ªä¸ªæä¾›å•†ï¼Œåœ¨æ‚¨çš„é…ç½®ä¸­æ·»åŠ `provider_type`å‚æ•°ï¼š

```yaml
# å¯¹äºE2B
provider_type: e2b

# å¯¹äºMorph
provider_type: morph
```

##### æ•°æ®é›†è¦æ±‚

ç¡®ä¿æ‚¨çš„æ•°æ®é›†åŒ…å«å…·æœ‰ä»¥ä¸‹æ¨¡å¼çš„`verification_info`åˆ—ï¼ˆé‡‡ç”¨PrimeIntellectä¼˜ç§€çš„å¯éªŒè¯é—®é¢˜[æ•°æ®é›†](https://huggingface.co/collections/PrimeIntellect/synthetic-1-67a2c399cfdd6c9f7fae0c37)ï¼‰ï¼š

```python
{
    "language": "python",  # Morphæ”¯æŒæ›´å¤šè¯­è¨€ï¼ŒåŒ…æ‹¬C++ã€Javaç­‰ã€‚
    "test_cases": [
        {
            "input": "4\n4\n0001\n1000\n0011\n0111\n3\n010\n101\n0\n2\n00000\n00001\n4\n01\n001\n0001\n00001\n",
            "output": "1\n3 \n-1\n0\n\n2\n1 2 \n",
            "type": "stdin_stdout",
        }
    ],
}
```

ä¾‹å¦‚ï¼Œè¦åœ¨Pythoné—®é¢˜ä¸Šè®­ç»ƒå°å‹æ¨¡å‹ï¼Œå¯åŠ¨vLLMæœåŠ¡å™¨ï¼š

```shell
CUDA_VISIBLE_DEVICES=0 trl vllm-serve --model Qwen/Qwen2.5-1.5B-Instruct
```

ç„¶åè¿è¡Œè®­ç»ƒï¼š

```shell
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 ACCELERATE_LOG_LEVEL=info \
    accelerate launch --config_file recipes/accelerate_configs/zero2.yaml --num_processes=7 \
    src/open_r1/grpo.py --config recipes/Qwen2.5-1.5B-Instruct/grpo/config_demo_code.yaml
```

##### ä½¿ç”¨è·¯ç”±å™¨æœåŠ¡

å½“åœ¨æ²™ç®±æœåŠ¡ä¸Šæ‰§è¡Œå¤ªå¤šè„šæœ¬æ—¶ï¼Œå¯èƒ½ä¼šå—åˆ°é€Ÿç‡é™åˆ¶ã€‚å¯¹äºä¸¤ä¸ªæä¾›å•†ï¼Œæˆ‘ä»¬æä¾›å¯ä»¥åœ¨CPUèŠ‚ç‚¹ä¸Šå¯åŠ¨çš„è·¯ç”±å™¨è„šæœ¬ï¼š

å¯¹äºE2Bï¼š
```shell
sbatch slurm/e2b_router.slurm
```

å¯¹äºMorphï¼š
```shell
sbatch slurm/morph_router.slurm
```

ç„¶ååœ¨æ‚¨çš„è®­ç»ƒYAMLé…ç½®ä¸­æ·»åŠ è·¯ç”±å™¨URLï¼š
```yaml
# å¯¹äºE2B
e2b_router_url: 1.2.3.4:8000

# å¯¹äºMorph
morph_router_url: 1.2.3.4:8000
```

ç«¯å£åº”ä¸å¯åŠ¨è·¯ç”±å™¨æ—¶ä½¿ç”¨çš„ç«¯å£åŒ¹é…ã€‚
æ‰€æœ‰è®­ç»ƒä½œä¸šå¯ä»¥å…±äº«ç›¸åŒçš„è·¯ç”±å™¨IPï¼Œè¿™å°†ç¡®ä¿å¹¶è¡Œæ‰§è¡Œå¾—åˆ°æ­£ç¡®ç®¡ç†ã€‚

#### ç«æŠ€ç¼–ç¨‹é—®é¢˜ï¼šIOIå’ŒCodeForces

æˆ‘ä»¬ä¸ºæ‰§è¡Œæ¥è‡ª[IOI](https://hf.co/datasets/open-r1/ioi)å’Œ[CodeForces](https://huggingface.co/datasets/open-r1/codeforces)çš„é—®é¢˜åˆ†åˆ«æä¾›äº†`ioi_code_reward`å’Œ`cf_code_reward`å¥–åŠ±å‡½æ•°ã€‚æ‚¨å¯ä»¥ä½¿ç”¨[piston](https://github.com/engineer-man/piston)æˆ–Morphï¼ˆç›®å‰ä»…IOIï¼‰ä½œä¸ºæ‚¨çš„æ‰§è¡Œæä¾›å•†ã€‚

##### Piston

è¦ä½¿ç”¨Pistonï¼š
1. è®©pistonå·¥ä½œå™¨è¿è¡Œï¼Œè¯·å‚é˜…[slurm/piston/README.md](./slurm/piston/README.md)
2. å°†æ‚¨çš„ç¯å¢ƒå˜é‡`PISTON_ENDPOINTS`è®¾ç½®ä¸º`slurm`æˆ–pistonå·¥ä½œå™¨ç«¯ç‚¹åˆ—è¡¨

å¯¹äºIOIï¼š

3. åœ¨æ‚¨çš„é…ç½®ä¸­ï¼Œä½¿ç”¨`ioi_provider: "piston"`

å¯¹äºCodeForcesï¼š

3. ä¸‹è½½ç”Ÿæˆçš„ï¼ˆå›°éš¾ï¼‰æµ‹è¯•ç”¨ä¾‹ï¼š
```
# æ›´æ”¹PATH_TO_SAVE_TESTCASESã€‚æ ¹æ®æ‚¨æœºå™¨çš„å®¹é‡å¢åŠ --max-workers
huggingface-cli download open-r1/codeforces --repo-type=dataset --include='generated_tests/*.parquet' --max-workers=8 --local-dir PATH_TO_SAVE_TESTCASES 
```
4. å°†è·¯å¾„ä¿å­˜åœ¨.envä¸­ï¼š
```
CF_TESTS_FOLDER=PATH_TO_SAVE_TESTCASES
```

##### Morph

Morphæ˜¯ä¸€ä¸ªåŸºäºäº‘çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ºè¿è¡Œä»£ç æä¾›æ²™ç®±ç¯å¢ƒã€‚è¦ä½¿ç”¨å®ƒï¼š
1. å®‰è£…Morphå®¢æˆ·ç«¯ï¼š`pip install morphcloud`
2. å°†æ‚¨çš„Morph APIå¯†é’¥æ·»åŠ åˆ°`.env`æ–‡ä»¶ï¼š`MORPH_API_KEY="your_key_here"`
3. åœ¨æ‚¨çš„é…ç½®ä¸­ï¼Œä½¿ç”¨`ioi_provider: "morph"`

##### ç¤ºä¾‹é…æ–¹
å¯¹äºIOIï¼š

è¯·å‚é˜…[ç¤ºä¾‹é…æ–¹](./recipes/Qwen2.5-1.5B-Instruct/grpo/config_demo_code_ioi.yaml)äº†è§£å¦‚ä½•ä½¿ç”¨IOIå¥–åŠ±å‡½æ•°ï¼š

```shell
ACCELERATE_LOG_LEVEL=info accelerate launch --config_file recipes/accelerate_configs/zero2.yaml \
    --num_processes=7 src/open_r1/grpo.py \
    --config recipes/Qwen2.5-1.5B-Instruct/grpo/config_demo_code_ioi.yaml
```

å¯¹äºCodeForcesï¼š

```shell
sbatch --job-name=cf-grpo --nodes=2 slurm/train.slurm --model Qwen2.5-Coder-7B-Instruct --task grpo --config codeforces --accelerator zero3 --dp 8 --tp 1
```

### åœ¨Slurmé›†ç¾¤ä¸Šå¯åŠ¨ä½œä¸š

å¦‚æœæ‚¨æœ‰è®¿é—®Slurmé›†ç¾¤çš„æƒé™ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ª`slurm/train.slurm`è„šæœ¬ï¼Œå®ƒå°†è‡ªåŠ¨ä¸ºæ‚¨æ’é˜Ÿè®­ç»ƒä½œä¸šã€‚ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨å®ƒï¼š

```shell
sbatch --job-name=open_r1 --nodes=1 slurm/train.slurm --model {model_name} --task {task} --config {config_suffix} --accelerator {accelerator}
```

è¿™é‡Œ`{model_name}`å’Œ`{task}`å¦‚ä¸Šæ‰€å®šä¹‰ï¼Œè€Œ`{config_suffix}`æŒ‡çš„æ˜¯ç‰¹å®šé…ç½®ï¼Œ`{accelerator}`æŒ‡çš„æ˜¯`recipes/accelerate_configs`ä¸­ğŸ¤— Accelerateé…ç½®çš„é€‰æ‹©ã€‚å¦‚æœæ‚¨å¸Œæœ›è¦†ç›–é»˜è®¤é…ç½®å‚æ•°ï¼Œå¯ä»¥é€šè¿‡é™„åŠ ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²æ¥æä¾›å®ƒä»¬ï¼Œå¦‚`'--arg1=value1 --arg2=value2'`ã€‚ä»¥ä¸‹æ˜¯åœ¨8ä¸ªGPUçš„1ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡ŒSFTçš„å…·ä½“ç¤ºä¾‹ï¼š

```shell
sbatch --job-name=open_r1 --nodes=1 slurm/train.slurm --model OpenR1-Distill-7B --task sft --config distill --accelerator zero3
```

æ‚¨å¯ä»¥é€šè¿‡å¢åŠ `--nodes`æ ‡å¿—æ¥æ‰©å±•èŠ‚ç‚¹æ•°é‡ã€‚

å¯¹äºGRPOï¼Œæˆ‘ä»¬ä½¿ç”¨1ä¸ªèŠ‚ç‚¹ç”¨äºvLLMæœåŠ¡å™¨ï¼ŒNä¸ªèŠ‚ç‚¹ç”¨äºè®­ç»ƒã€‚ä¾‹å¦‚ï¼Œè¦åœ¨1+1ä¸ªèŠ‚ç‚¹ä¸Šä½¿ç”¨æ··åˆæ•°æ®å’Œå¼ é‡å¹¶è¡Œè¿è¡ŒGRPOï¼Œè¿è¡Œï¼š

```shell
sbatch --job-name=open_r1 --nodes=2 slurm/train.slurm --model Qwen2.5-1.5B-Instruct --task grpo --config demo --accelerator zero2 --dp 4 --tp 2
```

> [!NOTE]
> `slurm/train.slurm`ä¸­çš„é…ç½®é’ˆå¯¹Hugging Faceè®¡ç®—é›†ç¾¤è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´ä»¥é€‚åº”æ‚¨è‡ªå·±çš„è®¡ç®—èŠ‚ç‚¹ã€‚

### è‡ªå®šä¹‰æ•°æ®é›†æ··åˆ

è¦å°†å¤šä¸ªæ•°æ®é›†ç»„åˆä¸ºå•ä¸ªè®­ç»ƒæ··åˆï¼Œæ‚¨å¯ä»¥åœ¨YAMLé…ç½®æ–‡ä»¶ä¸­æŒ‡å®š`dataset_mixture`å‚æ•°ã€‚ä»¥ä¸‹æ˜¯æ‰§è¡Œæ­¤æ“ä½œçš„æ¨¡æ¿ï¼š

```yaml
dataset_mixture:
  datasets:                     # è¦åŒ…å«åœ¨æ··åˆä¸­çš„æ•°æ®é›†åˆ—è¡¨
    - id: dataset_1             # Hubæ•°æ®é›†ID
      config: config_name_1     # æ•°æ®é›†é…ç½®çš„åç§°
      split: split_1            # è¦ä»æ•°æ®é›†ä½¿ç”¨çš„åˆ†å‰²
      columns:                  # è¦ä¿ç•™çš„åˆ—
        - column_1              
        - column_2    
      weight: 0.25              # è¦ä½¿ç”¨çš„æ•°æ®é›†åˆ†æ•°
    - id: dataset_2
      config: config_name_2
      split: split_2
      columns:                  
        - column_1              
        - column_2   
      weight: 0.5
  seed: 42                      # ç”¨äºæ··æ´—ç»„åˆæ•°æ®é›†çš„ç§å­
  test_split_size: 0.1          # ç”¨äºæµ‹è¯•åˆ†å‰²çš„æ··åˆåˆ†æ•°
```

## è¯„ä¼°æ¨¡å‹

æˆ‘ä»¬ä½¿ç”¨`lighteval`æ¥è¯„ä¼°æ¨¡å‹ã€‚å¯¹äºé€‚åˆå•ä¸ªGPUçš„æ¨¡å‹ï¼Œè¿è¡Œï¼š

```shell
export VLLM_WORKER_MULTIPROC_METHOD=spawn # vLLMæ‰€éœ€
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=data/evals/$MODEL

# AIME 2024
TASK=aime24
lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR

# MATH-500
TASK=math_500
lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR

# GPQA Diamond
TASK=gpqa:diamond
lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR

# LiveCodeBench
lighteval vllm $MODEL_ARGS "extended|lcb:codegeneration|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR 
```

è¦åœ¨å¤šä¸ªGPUä¸Šå¢åŠ ååé‡ï¼Œä½¿ç”¨_æ•°æ®å¹¶è¡Œ_å¦‚ä¸‹ï¼š

```shell
NUM_GPUS=8
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,data_parallel_size=$NUM_GPUS,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
TASK=aime24
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

å¯¹äºéœ€è¦è·¨GPUåˆ†ç‰‡çš„å¤§å‹æ¨¡å‹ï¼Œä½¿ç”¨_å¼ é‡å¹¶è¡Œ_å¹¶è¿è¡Œï¼š

```shell
NUM_GPUS=8
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,tensor_parallel_size=$NUM_GPUS,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
TASK=aime24
OUTPUT_DIR=data/evals/$MODEL

export VLLM_WORKER_MULTIPROC_METHOD=spawn
lighteval vllm $MODEL_ARGS "lighteval|$TASK|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨`make evaluate`å¯åŠ¨è¯„ä¼°ï¼ŒæŒ‡å®šæ¨¡å‹ã€ä»»åŠ¡å’Œå¯é€‰çš„å¹¶è¡ŒæŠ€æœ¯å’ŒGPUæ•°é‡ã€‚

è¦åœ¨å•ä¸ªGPUä¸Šè¯„ä¼°ï¼š

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24
```

è¦ä½¿ç”¨æ•°æ®å¹¶è¡Œï¼š

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24 PARALLEL=data NUM_GPUS=8
```

è¦ä½¿ç”¨å¼ é‡å¹¶è¡Œï¼š

```shell
make evaluate MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-32B TASK=aime24 PARALLEL=tensor NUM_GPUS=8
```

## å¤ç°Deepseekçš„è¯„ä¼°ç»“æœ

DeepSeek-R1è®ºæ–‡ä½¿ç”¨æ¯ä¸ªæŸ¥è¯¢4-64ä¸ªå“åº”çš„é‡‡æ ·æ¥ä¼°è®¡`pass@1`å‡†ç¡®ç‡ï¼Œä½†æ²¡æœ‰æŒ‡å®šæ¯ä¸ªåŸºå‡†çš„å…·ä½“å“åº”æ•°é‡ã€‚åœ¨ä¸‹è¡¨ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ¯ä¸ªæŸ¥è¯¢çš„å“åº”æ•°é‡æ¥ä¼°è®¡`pass@1`å‡†ç¡®ç‡ï¼š

|   åŸºå‡†   | æ¯ä¸ªæŸ¥è¯¢çš„å“åº”æ•°é‡ |
|:-------------:|:-----------------------------:|
|   AIME 2024   |              64               |
|   MATH-500    |               4               |
| GPQA Diamond  |               8               |
| LiveCodeBench |              16               |

è¯·æ³¨æ„ï¼Œå¯¹äºåƒAIME24è¿™æ ·çš„åŸºå‡†ï¼Œé‡è¦çš„æ˜¯è¦é‡‡æ ·è®¸å¤šå“åº”ï¼Œå› ä¸ºåªæœ‰30ä¸ªé—®é¢˜ï¼Œè¿™å¯èƒ½åœ¨é‡å¤è¿è¡Œä¸­å¼•å…¥é«˜æ–¹å·®ã€‚æ¯ä¸ªæç¤ºé‡‡æ ·å¤šå°‘å“åº”çš„é€‰æ‹©å¯èƒ½è§£é‡Šäº†æˆ‘ä»¬çš„è¯„ä¼°ç»“æœä¸DeepSeekæŠ¥å‘Šçš„ç»“æœä¹‹é—´çš„å°å·®å¼‚ã€‚

### AIME 2024

æˆ‘ä»¬èƒ½å¤Ÿåœ¨çº¦1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨AIME 2024åŸºå‡†ä¸Šçš„æŠ¥å‘Šç»“æœï¼š

| æ¨¡å‹                         | AIME 2024 (ğŸ¤— LightEval) | AIME 2024 (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:------------------------:|:-----------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |           30.7           |             28.9              |
| DeepSeek-R1-Distill-Qwen-7B   |           50.8           |             55.5              |
| DeepSeek-R1-Distill-Qwen-14B  |           65.9           |             69.7              |
| DeepSeek-R1-Distill-Qwen-32B  |           69.7           |             72.6              |
| DeepSeek-R1-Distill-Llama-8B  |           43.9           |             41.7              |
| DeepSeek-R1-Distill-Llama-70B |           63.0           |             70.0              |

è¦å¤ç°è¿™äº›ç»“æœï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```shell
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8
MODEL=deepseek-ai/{model_name}
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,data_parallel_size=$NUM_GPUS,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "lighteval|aime24|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

æˆ–è€…ï¼Œæ‚¨å¯ä»¥å¦‚ä¸‹å¯åŠ¨Slurmä½œä¸šï¼š

```shell
python scripts/run_benchmarks.py --model-id {model_id}  --benchmarks aime24
```

### MATH-500

æˆ‘ä»¬èƒ½å¤Ÿåœ¨çº¦1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨MATH-500åŸºå‡†ä¸Šçš„æŠ¥å‘Šç»“æœï¼š

| æ¨¡å‹                         | MATH-500 (ğŸ¤— LightEval) | MATH-500 (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:-----------------------:|:----------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |          83.1           |             83.9             |
| DeepSeek-R1-Distill-Qwen-7B   |          94.5           |             92.8             |
| DeepSeek-R1-Distill-Qwen-14B  |          94.1           |             93.9             |
| DeepSeek-R1-Distill-Qwen-32B  |          95.6           |             94.3             |
| DeepSeek-R1-Distill-Llama-8B  |          88.6           |             89.1             |
| DeepSeek-R1-Distill-Llama-70B |          95.1           |             94.5             |

è¦å¤ç°è¿™äº›ç»“æœï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```shell
export VLLM_WORKER_MULTIPROC_METHOD=spawn
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8
MODEL=deepseek-ai/{model_name}
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,data_parallel_size=$NUM_GPUS,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "lighteval|math_500|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

æˆ–è€…ï¼Œæ‚¨å¯ä»¥å¦‚ä¸‹å¯åŠ¨Slurmä½œä¸šï¼š

```shell
python scripts/run_benchmarks.py --model-id {model_id}  --benchmarks math_500
```

### GPQA Diamond

æˆ‘ä»¬èƒ½å¤Ÿåœ¨çº¦1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨GPQA DiamondåŸºå‡†ä¸Šçš„æŠ¥å‘Šç»“æœï¼š

| æ¨¡å‹                         | GPQA Diamond (ğŸ¤— LightEval) | GPQA Diamond (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:---------------------------:|:--------------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |            35.8             |               33.8               |
| DeepSeek-R1-Distill-Qwen-7B   |            50.5             |               49.1               |
| DeepSeek-R1-Distill-Qwen-14B  |            61.5             |               59.1               |
| DeepSeek-R1-Distill-Qwen-32B  |            63.1             |               62.1               |
| DeepSeek-R1-Distill-Llama-8B  |            46.7             |               49.0               |
| DeepSeek-R1-Distill-Llama-70B |            67.4             |               65.2               |

è¦å¤ç°è¿™äº›ç»“æœï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```shell
export VLLM_WORKER_MULTIPROC_METHOD=spawn
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8
MODEL=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "lighteval|gpqa:diamond|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

```shell
python scripts/run_benchmarks.py --model-id {model_id}  --benchmarks gpqa
```

### LiveCodeBench

æˆ‘ä»¬èƒ½å¤Ÿåœ¨çº¦1-3ä¸ªæ ‡å‡†å·®å†…å¤ç°Deepseekåœ¨LiveCodeBenchä»£ç ç”ŸæˆåŸºå‡†ä¸Šçš„æŠ¥å‘Šç»“æœï¼š

| æ¨¡å‹                         | LiveCodeBench (ğŸ¤— LightEval) | LiveCodeBench (DeepSeekæŠ¥å‘Š) |
|:------------------------------|:----------------------------:|:---------------------------------:|
| DeepSeek-R1-Distill-Qwen-1.5B |             16.1             |               16.9                |
| DeepSeek-R1-Distill-Qwen-7B   |             37.4             |               37.6                |
| DeepSeek-R1-Distill-Qwen-14B  |             51.3             |               53.1                |
| DeepSeek-R1-Distill-Qwen-32B  |             56.0             |             57.2                |
| DeepSeek-R1-Distill-Llama-8B  |             37.4             |               39.6                |
| DeepSeek-R1-Distill-Llama-70B |             55.9             |               57.5                |

è¦å¤ç°è¿™äº›ç»“æœï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```shell
NUM_GPUS=1 # å¯¹äº32Bå’Œ70Bæ¨¡å‹è®¾ç½®ä¸º8ï¼Œæˆ–å¯¹äºè¾ƒå°æ¨¡å‹ä½¿ç”¨data_parallel_size=8ä»¥æé«˜é€Ÿåº¦
MODEL=deepseek-ai/{model_name}
MODEL_ARGS="model_name=$MODEL,dtype=bfloat16,max_model_length=32768,gpu_memory_utilization=0.8,data_parallel_size=$NUM_GPUS,generation_parameters={max_new_tokens:32768,temperature:0.6,top_p:0.95}"
OUTPUT_DIR=data/evals/$MODEL

lighteval vllm $MODEL_ARGS "extended|lcb:codegeneration|0|0" \
    --use-chat-template \
    --output-dir $OUTPUT_DIR
```

```shell
python scripts/run_benchmarks.py --model-id {model_id}  --benchmarks lcb
```

## æ•°æ®ç”Ÿæˆ

### ä»å°å‹è’¸é¦R1æ¨¡å‹ç”Ÿæˆæ•°æ®

ä»¥ä¸‹ç¤ºä¾‹å¯ä»¥åœ¨1xH100ä¸Šè¿è¡Œã€‚
é¦–å…ˆå®‰è£…ä»¥ä¸‹ä¾èµ–é¡¹ï¼š

```shell
uv pip install "distilabel[vllm]>=1.5.2"
```

ç°åœ¨å°†ä»¥ä¸‹ä»£ç ç‰‡æ®µä¿å­˜åˆ°åä¸º`pipeline.py`çš„æ–‡ä»¶ä¸­ï¼Œå¹¶ä½¿ç”¨`python pipeline.py`è¿è¡Œå®ƒã€‚å®ƒå°†ä¸º10ä¸ªç¤ºä¾‹ä¸­çš„æ¯ä¸€ä¸ªç”Ÿæˆ4ä¸ªè¾“å‡ºï¼ˆå°†å­˜å‚¨åº“çš„ç”¨æˆ·åæ›´æ”¹ä¸ºæ‚¨çš„ç»„ç»‡/ç”¨æˆ·åï¼‰ï¼š

```python
from datasets import load_dataset
from distilabel.models import vLLM
from distilabel.pipeline import Pipeline
from distilabel.steps.tasks import TextGeneration


prompt_template = """\
æ‚¨å°†å¾—åˆ°ä¸€ä¸ªé—®é¢˜ã€‚è¯·é€æ­¥æ¨ç†ï¼Œå¹¶å°†æ‚¨çš„æœ€ç»ˆç­”æ¡ˆæ”¾åœ¨\boxed{}ä¸­ï¼š
{{ instruction }}"""

dataset = load_dataset("AI-MO/NuminaMath-TIR", split="train").select(range(10))

model_id = "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"  # ä¸å¦ä¸€ä¸ªå°å‹è’¸é¦r1äº¤æ¢

with Pipeline(
    name="distill-qwen-7b-r1",
    description="ä»è’¸é¦r1æ¨¡å‹ç”Ÿæˆæ•°æ®çš„æµæ°´çº¿",
) as pipeline:

    llm = vLLM(
        model=model_id,
        tokenizer=model_id,
        extra_kwargs={
            "tensor_parallel_size": 1,
            "max_model_len": 8192,
        },
        generation_kwargs={
            "temperature": 0.6,
            "max_new_tokens": 8192,
        },
    )
    prompt_column = "problem"
    text_generation = TextGeneration(
        llm=llm, 
        template=prompt_template,
        num_generations=4,
        input_mappings={"instruction": prompt_column} if prompt_column is not None else {}
    )


if __name__ == "__main__":
    distiset = pipeline.run(dataset=dataset)
    distiset.push_to_hub(repo_id="username/numina-deepseek-r1-qwen-7b")
```

æŸ¥çœ‹[HuggingFaceH4/numina-deepseek-r1-qwen-7b](https://huggingface.co/datasets/HuggingFaceH4/numina-deepseek-r1-qwen-7b)çš„ç¤ºä¾‹æ•°æ®é›†ã€‚

### ä»DeepSeek-R1ç”Ÿæˆæ•°æ®

è¦è¿è¡Œæ›´å¤§çš„DeepSeek-R1ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†2ä¸ªèŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹æœ‰8Ã—H100 GPUï¼Œä½¿ç”¨æ­¤å­˜å‚¨åº“ä¸­çš„slurmæ–‡ä»¶`slurm/generate.slurm`ã€‚é¦–å…ˆï¼Œå®‰è£…ä¾èµ–é¡¹ï¼š

ï¼ˆç°åœ¨æˆ‘ä»¬éœ€è¦å®‰è£…[ä¿®å¤R1 cudaå›¾æ•è·](https://github.com/vllm-project/vllm/commits/221d388cc5a836fa189305785ed7e887cea8b510/csrc/moe/moe_align_sum_kernels.cu)çš„vllm dev wheelï¼‰
```shell
pip install https://wheels.vllm.ai/221d388cc5a836fa189305785ed7e887cea8b510/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu121

uv pip install "distilabel[vllm,ray,openai]>=1.5.2"
```

ç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
sbatch slurm/generate.slurm \
    --hf-dataset AI-MO/NuminaMath-TIR \
    --temperature 0.6 \
    --prompt-column problem \
    --model deepseek-ai/DeepSeek-R1 \
    --hf-output-dataset username/r1-dataset
```

> [!NOTE]  
> å½“ä½œä¸šè¿è¡Œæ—¶ï¼Œæ‚¨å¯ä»¥è®¾ç½®é€šè¿‡é›†ç¾¤ç™»å½•èŠ‚ç‚¹çš„SSHéš§é“ï¼Œé€šè¿‡è¿è¡Œ`ssh -L 8265:ray_ip_head_node:8265 <login_node>`ä»æ‚¨çš„è®¡ç®—æœºè®¿é—®Rayä»ªè¡¨æ¿ï¼Œç„¶åæµè§ˆ`http://localhost:8265`

### æ•°æ®å»æ±¡æŸ“

éµå¾ª[s1: Simple test-time scaling](https://huggingface.co/papers/2501.19393)ï¼Œå¯ä»¥ä½¿ç”¨ä½äº[scripts/decontaminate.py](./scripts/decontaminate.py)çš„è„šæœ¬å¯¹æ•°æ®è¿›è¡Œå»æ±¡æŸ“ï¼Œè¯¥è„šæœ¬ä½¿ç”¨8-gramå¯¹æ•°æ®é›†è¿›è¡Œå»æ±¡æŸ“å¹¶å»é‡ã€‚ç¤ºä¾‹è¿è¡Œï¼š

```shell
python scripts/decontaminate.py \
    --dataset "open-r1/verifiable-coding-problems-python" \
    --problem_column problem \
    --cleanup
```

å®ƒå°†é’ˆå¯¹åŸºå‡†æ•°æ®é›†è¿›è¡Œå»æ±¡æŸ“ï¼Œå¹¶ä¹‹ååˆ é™¤æ±¡æŸ“çš„æ ·æœ¬ã€‚å¦‚æœæ²¡æœ‰æä¾›å‚æ•°`--new_dataset_name`ï¼Œå°†é‡ç”¨ç›¸åŒçš„æ•°æ®é›†ï¼Œæ·»åŠ `_decontaminated`ã€‚å®ƒé’ˆå¯¹æç¤ºè¿è¡Œï¼Œå¯¹äºæ­¤æ•°æ®é›†æ˜¯åˆ—`problem`ï¼Œä½†å¯ä»¥æä¾›ä¸åŒçš„åˆ—ã€‚

è„šæœ¬çš„å‚æ•°ï¼š

```shell
usage: decontaminate.py [-h] --dataset DATASET [--split SPLIT] [--ngram_size NGRAM_SIZE] [--problem_column PROBLEM_COLUMN] [--cleanup] [--new_dataset_name NEW_DATASET_NAME]

options:
  -h, --help            æ˜¾ç¤ºæ­¤å¸®åŠ©æ¶ˆæ¯å¹¶é€€å‡º
  --dataset DATASET     è¦æ£€æŸ¥æ±¡æŸ“çš„æ•°æ®é›†åç§°ã€‚
  --split SPLIT         è¦æ£€æŸ¥æ±¡æŸ“çš„åˆ†å‰²ï¼Œé»˜è®¤ä¸º`train`ã€‚
  --ngram_size NGRAM_SIZE
                        è¦æ„å»ºçš„n-gramå¤§å°ï¼Œé»˜è®¤ä¸º8ã€‚
  --problem_column PROBLEM_COLUMN
                        åŒ…å«é—®é¢˜ï¼ˆæç¤ºï¼‰çš„åˆ—çš„åç§°ã€‚
  --cleanup           æ˜¯å¦åœ¨æ¨é€æ•°æ®é›†ä¹‹å‰åˆ é™¤æ±¡æŸ“çš„è¡Œã€‚
  --new_dataset_name NEW_DATASET_NAME
                        æ•°æ®é›†çš„æ–°åç§°ã€‚å¦‚æœæœªæä¾›ï¼Œå°†é‡ç”¨åç§°å¹¶åœ¨åç§°ä¸­æ·»åŠ `_decontaminated`ã€‚
```

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ã€‚è¯·å‚è€ƒhttps://github.com/huggingface/open-r1/issues/23ã€‚

## è‡´è°¢

è¯¥é¡¹ç›®æ˜¯é€šè¿‡å¼€æºAIç¤¾åŒºä¸­è®¸å¤šå›¢ä½“å’Œä¸ªäººçš„é›†ä½“åŠªåŠ›æ„å»ºçš„ã€‚æˆ‘ä»¬ç‰¹åˆ«æ„Ÿè°¢vLLMå’ŒSGLangå›¢é˜Ÿåˆ›å»ºäº†é«˜æ€§èƒ½å·¥å…·æ¥æ‰©å±•GRPOçš„æ¨å‡ºã€‚æˆ‘ä»¬è¿˜æ„Ÿè°¢[OpenThoughts](https://www.open-thoughts.ai)ã€[Prime Intellect](https://www.primeintellect.ai)å’Œ[General Reasoning](https://gr.inc)å›¢é˜Ÿåˆ›å»ºå’Œåˆ†äº«é«˜è´¨é‡çš„æ¨ç†æ•°æ®é›†ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨å‘ç°æ­¤é¡¹ç›®åœ¨æ‚¨è‡ªå·±çš„å·¥ä½œä¸­æœ‰ç”¨ï¼Œè¯·è€ƒè™‘å¦‚ä¸‹å¼•ç”¨ï¼š

```
@misc{openr1,
    title = {Open R1: å®Œå…¨å¼€æºçš„DeepSeek-R1å¤ç°},
    url = {https://github.com/huggingface/open-r1},
    author = {{Hugging Face}},
    month = {January},
    year = {2025}
}
```
